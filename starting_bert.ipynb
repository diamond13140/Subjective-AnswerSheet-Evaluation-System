{"cells":[{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1715256837989,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"Moq5gpYclmB0"},"outputs":[],"source":["from google.colab import drive"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5766,"status":"ok","timestamp":1715256843749,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"WxCTQiDnl8cA","outputId":"aa5fd48f-f097-4246-ca88-36ed02124333"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5517,"status":"ok","timestamp":1715256849248,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"1-XPDU9stxZz","outputId":"a2cb4c5a-cf44-4e2c-e721-f9aa0599e612"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"]}],"source":["!pip install tensorflow"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6644,"status":"ok","timestamp":1715256855847,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"nVvJ2IqjwWRR","outputId":"1b328698-1b5a-4ce9-898c-6e5b535ba603"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2024.2.2)\n"]}],"source":["!pip install transformers==4.31.0"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1715256855848,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"CGENIztKmI9u"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import transformers"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1715256855848,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"sCWskpsWBet0"},"outputs":[],"source":["max_length = 128  # Maximum length of input sentence to the model.\n","batch_size = 32\n","epochs = 2"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1715256855850,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"VZ3djOzdBiYi"},"outputs":[],"source":["labels = [\"contradiction\", \"entailment\", \"neutral\"]"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1715256855851,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"9Am3zxYGBmtz"},"outputs":[],"source":["class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n","    \"\"\"Generates batches of data.\n","\n","    Args:\n","        sentence_pairs: Array of premise and hypothesis input sentences.\n","        labels: Array of labels.\n","        batch_size: Integer batch size.\n","        shuffle: boolean, whether to shuffle the data.\n","        include_targets: boolean, whether to incude the labels.\n","\n","    Returns:\n","        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n","        (or just `[input_ids, attention_mask, `token_type_ids]`\n","         if `include_targets=False`)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        sentence_pairs,\n","        labels,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        include_targets=True,\n","    ):\n","        self.sentence_pairs = sentence_pairs\n","        self.labels = labels\n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","        self.include_targets = include_targets\n","        # Load our BERT Tokenizer to encode the text.\n","        # We will use base-base-uncased pretrained model.\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n","            \"bert-base-uncased\", do_lower_case=True\n","        )\n","        self.indexes = np.arange(len(self.sentence_pairs))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        # Denotes the number of batches per epoch.\n","        return len(self.sentence_pairs) // self.batch_size\n","\n","    def __getitem__(self, idx):\n","        # Retrieves the batch of index.\n","        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n","        sentence_pairs = self.sentence_pairs[indexes]\n","\n","        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n","        # encoded together and separated by [SEP] token.\n","        encoded = self.tokenizer.batch_encode_plus(\n","            sentence_pairs.tolist(),\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_token_type_ids=True,\n","            pad_to_max_length=True,\n","            return_tensors=\"tf\",\n","        )\n","\n","        # Convert batch of encoded features to numpy array.\n","        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n","\n","        # Set to true if data generator is used for training/validation.\n","        if self.include_targets:\n","            labels = np.array(self.labels[indexes], dtype=\"int32\")\n","            return [input_ids, attention_masks, token_type_ids], labels\n","        else:\n","            return [input_ids, attention_masks, token_type_ids]\n","\n","    def on_epoch_end(self):\n","        # Shuffle indexes after each epoch if shuffle is set to True.\n","        if self.shuffle:\n","            np.random.RandomState(42).shuffle(self.indexes)\n","\n"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5251,"status":"ok","timestamp":1715256861087,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"lvFs7ASWBsLT","outputId":"55227e7e-c489-42b9-a20f-e141ba0e483f"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7dcabadaeaa0>\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_ids (InputLayer)      [(None, 128)]                0         []                            \n","                                                                                                  \n"," attention_masks (InputLaye  [(None, 128)]                0         []                            \n"," r)                                                                                               \n","                                                                                                  \n"," token_type_ids (InputLayer  [(None, 128)]                0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," bert (TFBertMainLayer)      TFBaseModelOutputWithPooli   1094822   ['input_ids[0][0]',           \n","                             ngAndCrossAttentions(last_   40         'attention_masks[0][0]',     \n","                             hidden_state=(None, 128, 7              'token_type_ids[0][0]']      \n","                             68),                                                                 \n","                              pooler_output=(None, 768)                                           \n","                             , past_key_values=None, hi                                           \n","                             dden_states=None, attentio                                           \n","                             ns=None, cross_attentions=                                           \n","                             None)                                                                \n","                                                                                                  \n"," bidirectional_1 (Bidirecti  (None, 128, 128)             426496    ['bert[0][0]']                \n"," onal)                                                                                            \n","                                                                                                  \n"," global_average_pooling1d_1  (None, 128)                  0         ['bidirectional_1[0][0]']     \n","  (GlobalAveragePooling1D)                                                                        \n","                                                                                                  \n"," global_max_pooling1d_1 (Gl  (None, 128)                  0         ['bidirectional_1[0][0]']     \n"," obalMaxPooling1D)                                                                                \n","                                                                                                  \n"," concatenate_1 (Concatenate  (None, 256)                  0         ['global_average_pooling1d_1[0\n"," )                                                                  ][0]',                        \n","                                                                     'global_max_pooling1d_1[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," dropout_112 (Dropout)       (None, 256)                  0         ['concatenate_1[0][0]']       \n","                                                                                                  \n"," dense_1 (Dense)             (None, 3)                    771       ['dropout_112[0][0]']         \n","                                                                                                  \n","==================================================================================================\n","Total params: 109909507 (419.27 MB)\n","Trainable params: 427267 (1.63 MB)\n","Non-trainable params: 109482240 (417.64 MB)\n","__________________________________________________________________________________________________\n"]}],"source":["strategy = tf.distribute.MirroredStrategy()\n","\n","with strategy.scope():\n","    # Encoded token ids from BERT tokenizer.\n","    input_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n","    )\n","    # Attention masks indicates to the model which tokens should be attended to.\n","    attention_masks = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n","    )\n","    # Token type ids are binary masks identifying different sequences in the model.\n","    token_type_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n","    )\n","    # Loading pretrained BERT model.\n","    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n","    # Freeze the BERT model to reuse the pretrained features without modifying them.\n","    bert_model.trainable = False\n","\n","    bert_output = bert_model.bert(\n","        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n","    )\n","    sequence_output = bert_output.last_hidden_state\n","    pooled_output = bert_output.pooler_output\n","    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n","    bi_lstm = tf.keras.layers.Bidirectional(\n","        tf.keras.layers.LSTM(64, return_sequences=True)\n","    )(sequence_output)\n","    # Applying hybrid pooling approach to bi_lstm sequence output.\n","    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n","    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n","    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n","    dropout = tf.keras.layers.Dropout(0.3)(concat)\n","    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n","    model = tf.keras.models.Model(\n","        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n","    )\n","\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(),\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"acc\"],\n","    )\n","\n","\n","print(f\"Strategy: {strategy}\")\n","model.summary()\n"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":10183,"status":"ok","timestamp":1715256871248,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"YQ8Q9RMwFX9l"},"outputs":[],"source":["from tensorflow import keras\n","\n","loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/Bert/loaded_model.h5')\n","\n","def check_similarity(sentence1, sentence2):\n","    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n","    test_data = BertSemanticDataGenerator(\n","        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n","    )\n","\n","    proba = loaded_model.predict(test_data[0])[0]\n","    idx = np.argmax(proba)\n","    proba = f\"{proba[idx]: .2f}%\"\n","    pred = labels[idx]\n","    return pred, proba\n"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1715256871249,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"2ty84eE4fiIy"},"outputs":[],"source":["import re\n","import warnings\n","import pandas as pd\n","\n","def calculate_similarity_scores(evaluator_content, student_content):\n","    def create_dictionary(content):\n","        # Remove numeric indices before \"Question\" and \"Answer\" words, preserving the colon\n","        content_without_indices = re.sub(r'(\\b\\d+\\.\\s*Question:)', 'Question:', content)\n","        content_without_indices = re.sub(r'(\\b\\d+\\.\\s*Answer:)', 'Answer:', content_without_indices)\n","\n","        # Tokenize and print each word with punctuation\n","        words = re.findall(r'\\b\\w+\\b|[.,;!?:]', content_without_indices)\n","\n","        sentence = \"\"\n","        my_dic = {}\n","        n = 0\n","\n","        for word in words:\n","            if word == \"Answer\" and words[n + 1] == \":\":\n","                key = sentence.strip()\n","                sentence = \"\"\n","            if word == \"Question\" and n > 4 and words[n + 1] == \":\":\n","                my_dic[key] = sentence.strip()\n","                sentence = \"\"\n","            sentence += word\n","            sentence += \" \"\n","            n += 1\n","\n","        # Check for the last question-answer pair\n","        if sentence.strip() and key:\n","            my_dic[key] = sentence.strip()\n","\n","        return my_dic\n","\n","    eval_dic = create_dictionary(evaluator_content)\n","    std_dic = create_dictionary(student_content)\n","\n","    data = {'Evaluator': [], 'Student': [], 'per_mark': [], 'Similarity': []}\n","    total_entailment = 0\n","    total_neutral = 0\n","    total_contradiction = 0\n","\n","    for fn in eval_dic.keys():\n","        sentence_1 = eval_dic[fn]\n","        sentence_2 = std_dic[fn]\n","        result = check_similarity(sentence_1, sentence_2)\n","\n","        # Extracting percentage value from the tuple\n","        percentage = float(result[1].strip('%'))\n","\n","        data['Student'].append(sentence_2)\n","        data['Evaluator'].append(sentence_1)\n","        data['per_mark'].append(percentage)\n","        data['Similarity'].append(result[0])\n","\n","        # Update total scores based on result type\n","        if result[0] == 'entailment':\n","            total_entailment += percentage\n","        elif result[0] == 'neutral':\n","            total_neutral += percentage / 3\n","        elif result[0] == 'contradiction':\n","            total_contradiction += percentage\n","\n","    # Calculate average scores\n","    num_entailment = len([x for x in data['Similarity'] if x == 'entailment'])\n","    num_neutral = len([x for x in data['Similarity'] if x == 'neutral'])\n","    num_contradiction = len([x for x in data['Similarity'] if x == 'contradiction'])\n","\n","    num = (num_entailment + num_neutral + num_contradiction)\n","    total_contradiction=0\n","    total_accuracy = (total_contradiction + total_entailment + total_neutral) / num\n","\n","    # Add total accuracy to DataFrame\n","    data['Total_Accuracy'] = total_accuracy\n","\n","    df = pd.DataFrame(data)\n","    df.to_csv('output.csv', index=False)\n","    return df\n"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":42,"status":"ok","timestamp":1715256871249,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"MRSdCmx-G0Ln"},"outputs":[],"source":["\n","# # Example inference\n","# sentence1 = \"Two women are observing something together.\"\n","# sentence2 = \"Two women are standing with their eyes closed.\"\n","# result = check_similarity(sentence1, sentence2)\n","# print(result)\n","\n","# sentence1 = \" The: greenhouse effect is a natural process where specific gases in Earth's atmosphere trap and radiate heat. Solar radiation passes through the atmosphere, warming the Earth's surface. The surface emits infrared radiation, but greenhouse gases like carbon dioxide and methane absorb and re-emit this radiation. This action traps heat, maintaining a temperature suitable for life.\"\n","# sentence2 = \" The: Carbon dioxide (CO2) is harmful to the environment mainly due to its contribution to the greenhouse effect. Excessive CO2, primarily from human activities like burning fossil fuels, intensifies the natural greenhouse effect, leading to global warming and climate change. This results in more frequent and severe weather events, disruptions to ecosystems, and rising sea levels.\"\n","# result = check_similarity(sentence1, sentence2)\n","# print(result)\n"]},{"cell_type":"code","execution_count":77,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":2393,"status":"ok","timestamp":1715257694269,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"F4u5X4s0mSpq","outputId":"d2c69b3b-ae46-4598-de34-d9fe3faabd74"},"outputs":[{"output_type":"stream","name":"stdout","text":["https://exonkop68xi-496ff2e9c6d22116-5000-colab.googleusercontent.com/\n"]}],"source":["from google.colab.output import eval_js\n","print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"]},{"cell_type":"markdown","source":[],"metadata":{"id":"UsJLzNpH_U35"}},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4360,"status":"ok","timestamp":1715256877016,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"SaedgTR1Fh6J","outputId":"d2950438-482e-4882-8171-3833b677d0c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":57,"status":"ok","timestamp":1715256877017,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"ivBt8tUkmXD2"},"outputs":[],"source":["from flask import Flask\n","from flask import request, redirect\n","from flask import render_template"]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":2146,"status":"ok","timestamp":1715259027683,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"Ug4qljKImg9i"},"outputs":[],"source":["app = Flask(__name__,template_folder = '/content/drive/MyDrive/Bert/templates')"]},{"cell_type":"code","execution_count":87,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1715259031019,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"zJm99dcemkSy"},"outputs":[],"source":["from IPython.display import clear_output\n","\n","\n","@app.route('/',methods=['GET','POST'])\n","def start():\n","    if request.method =='POST':\n","        if 'file1' in request.files and 'file2' in request.files:\n","            evaluator = request.files['file1']\n","            student = request.files['file2']\n","\n","            evaluator_path = \"/content/drive/MyDrive/Bert/history/evaluator.txt\"\n","            student_path = \"/content/drive/MyDrive/Bert/history/student.txt\"\n","\n","            evaluator.save(evaluator_path)\n","            student.save(student_path)\n","\n","            with open(evaluator_path, 'rb') as f1:\n","                    content1 = f1.read()\n","                    # evaluator_content = f1.read()\n","            evaluator = {'evaluator.txt': content1}\n","\n","            with open(student_path, 'rb') as f2:\n","                    content2 = f2.read()\n","                    # student_content = f2.read()\n","            student = {'student.txt': content2}\n","\n","            print(evaluator)\n","            print(student)\n","\n","            for ky in evaluator.keys():\n","                  evaluator_content = evaluator[ky].decode('utf-8')\n","            for ky in student.keys():\n","                  student_content = student[ky].decode('utf-8')\n","            clear_output()\n","            result_df = calculate_similarity_scores(evaluator_content, student_content)\n","            result_df.to_html('/content/drive/MyDrive/Bert/templates/output.html', index=False)\n","            print(result_df)\n","\n","            # result_df.to_csv(file_path, index=False).save(\"/content/drive/MyDrive/Bert/history/output.csv\")\n","            # result_df.to_csv(file_path, index=False)\n","            return render_template('output.html')\n","\n","        else:\n","            return \"<h3>Sorry we are unable to detect any files. Please Try Again</h3>\"\n","\n","    return render_template('index.html')"]},{"cell_type":"code","execution_count":62,"metadata":{"executionInfo":{"elapsed":52,"status":"ok","timestamp":1715256877018,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"t9e5wGujlRm6"},"outputs":[],"source":["# def output():\n","#   with open('/content/drive/MyDrive/Bert/history/evaluator.txt','rw') as f:\n","#     contents=f.read()\n","#     print(contents)"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":50,"status":"ok","timestamp":1715256877018,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"SK74dO_BVbwG"},"outputs":[],"source":["# !pip install --upgrade google-colab"]},{"cell_type":"code","execution_count":64,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1715256877019,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"u09VWqEhHZAE"},"outputs":[],"source":["\n","# evaluator_path = \"/content/drive/MyDrive/Bert/history/evaluator.txt\"\n","# student_path = \"/content/drive/MyDrive/Bert/history/student.txt\"\n","\n","\n","# with open(evaluator_path, 'rb') as file:\n","#         content = file.read()\n","\n","# evaluator = {'evaluator.txt': content}\n","\n","# with open(student_path, 'rb') as file:\n","#         content = file.read()\n","\n","# student = {'student.txt': content}\n","\n","\n","# for ky in evaluator.keys():\n","#       evaluator_content = evaluator[ky].decode('utf-8')\n","# for ky in student.keys():\n","#       student_content = student[ky].decode('utf-8')\n","\n","# evaluator_content\n","\n","# student_content"]},{"cell_type":"code","source":["# from google.colab import files\n","# evaluator = files.upload()\n","# student1 = files.upload()\n","# student2 = files.upload()\n","# student3 = files.upload()\n","# student4 = files.upload()\n","# for ky in evaluator.keys():\n","#       evaluator_content = evaluator[ky].decode('utf-8')\n","# for ky in student1.keys():\n","#       student_content1 = student1[ky].decode('utf-8')\n","# for ky in student2.keys():\n","#       student_content2= student2[ky].decode('utf-8')\n","# for ky in student3.keys():\n","#       student_content3= student3[ky].decode('utf-8')\n","# for ky in student4.keys():\n","#       student_content4 = student4[ky].decode('utf-8')"],"metadata":{"id":"E5muWpe97ihN","executionInfo":{"status":"ok","timestamp":1715256877019,"user_tz":-330,"elapsed":47,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# result_df = calculate_similarity_scores(evaluator_content, student_content1)\n","# result_df.to_csv('output.csv', index=False)\n","\n","# result_df1 = calculate_similarity_scores(evaluator_content, student_content2)\n","# result_df1.to_csv('output.csv', index=False)\n","\n","# result_df2 = calculate_similarity_scores(evaluator_content, student_content3)\n","# result_df2.to_csv('output.csv', index=False)\n","\n","# result_df3 = calculate_similarity_scores(evaluator_content, student_content4)\n","# result_df3.to_csv('output.csv', index=False)"],"metadata":{"id":"kZ8IunwT7wbZ","executionInfo":{"status":"ok","timestamp":1715256877020,"user_tz":-330,"elapsed":47,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# result_df"],"metadata":{"id":"nMavJ0K_8YIT","executionInfo":{"status":"ok","timestamp":1715256877020,"user_tz":-330,"elapsed":46,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["# result_df1"],"metadata":{"id":"4JVayszz8a7e","executionInfo":{"status":"ok","timestamp":1715256877020,"user_tz":-330,"elapsed":45,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["# result_df2"],"metadata":{"id":"sKv1Y3H_8dRS","executionInfo":{"status":"ok","timestamp":1715256877021,"user_tz":-330,"elapsed":45,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# result_df3"],"metadata":{"id":"4Yx57nk38eBu","executionInfo":{"status":"ok","timestamp":1715256877021,"user_tz":-330,"elapsed":44,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1715256877022,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"KMcT5avwH4j4"},"outputs":[],"source":["# print(\"TensorFlow version:\", tf.__version__)\n","# print(\"Transformers version:\", transformers.__version__)"]},{"cell_type":"code","execution_count":72,"metadata":{"executionInfo":{"elapsed":43,"status":"ok","timestamp":1715256877022,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"CtlKSGRBcXZ0"},"outputs":[],"source":["# sentence1=\"Answer : The water cycle , or hydrological cycle , consists of several stages . Evaporation occurs when water from the Earth s surface turns into vapor due to heat . Condensation involves the formation of clouds as water vapor cools and turns into liquid . Precipitation occurs when water droplets or ice crystals fall from clouds as rain , snow , sleet , or hail . Runoff involves the flow of water on the Earth s surface back into oceans , rivers , or lakes .\"\n","# sentence2=\"Answer : It is a cycle that made up of water \"\n","# result = check_similarity(sentence1, sentence2)\n","# print(result)"]},{"cell_type":"code","execution_count":88,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1715259035006,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"},"user_tz":-330},"id":"KVM45mTrwV5l"},"outputs":[],"source":["@app.route('/show')\n","def shows():\n","    return render_template('index.html')\n","    # return 'this is a product page'\n","\n","\n","@app.route('/about')\n","def about():\n","    return render_template('about.html')"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6upDZMOlwahH","outputId":"920e1ea6-e274-4f50-f202-befa8e25556d","executionInfo":{"status":"ok","timestamp":1715259509933,"user_tz":-330,"elapsed":473049,"user":{"displayName":"Abhay Sonwani","userId":"02351857348272779933"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 81ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 84ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 101ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 63ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 60ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 60ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 58ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 58ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 57ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 58ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [09/May/2024 12:56:23] \"POST / HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["                                           Evaluator  \\\n","0  Answer : Photosynthesis is the biological mech...   \n","1  Answer : Gravitational force is the attractive...   \n","2  Answer : DNA deoxyribonucleic acid is a double...   \n","3  Answer : The greenhouse effect is a natural ph...   \n","4  Answer : Magnetic fields are zones around a ma...   \n","5  Answer : Cellular respiration is the mechanism...   \n","6  Answer : The Doppler effect denotes the altera...   \n","7  Answer : Archimedes principle stipulates that ...   \n","8  Answer : The water cycle , or hydrological cyc...   \n","9  Answer : Neurons serve as the fundamental unit...   \n","\n","                                             Student  per_mark  Similarity  \\\n","0  Answer : Photosynthesis is the biological proc...      0.96  entailment   \n","1  Answer : Gravitational force is the attractive...      0.74  entailment   \n","2  Answer : DNA deoxyribonucleic acid is a double...      0.95  entailment   \n","3  Answer : The greenhouse effect is a natural pr...      0.93  entailment   \n","4  Answer : Magnetic fields are regions around a ...      0.91  entailment   \n","5  Answer : Cellular respiration is the process b...      0.94  entailment   \n","6  Answer : The Doppler effect is the change in f...      0.89  entailment   \n","7  Answer : Archimedes principle states that an o...      0.94  entailment   \n","8  Answer : The water cycle , or hydrological cyc...      0.80  entailment   \n","9  Answer : Neurons are the building blocks of th...      0.84  entailment   \n","\n","   Total_Accuracy  \n","0            0.89  \n","1            0.89  \n","2            0.89  \n","3            0.89  \n","4            0.89  \n","5            0.89  \n","6            0.89  \n","7            0.89  \n","8            0.89  \n","9            0.89  \n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [09/May/2024 12:56:43] \"GET / HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [09/May/2024 12:56:46] \"GET /about HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [09/May/2024 12:56:47] \"\u001b[33mGET /static/1712226771685.jpeg HTTP/1.1\u001b[0m\" 404 -\n","INFO:werkzeug:127.0.0.1 - - [09/May/2024 12:56:47] \"\u001b[33mGET /static/Mark_Zuckerberg.jpg HTTP/1.1\u001b[0m\" 404 -\n","INFO:werkzeug:127.0.0.1 - - [09/May/2024 12:56:47] \"\u001b[33mGET /static/sandeep-maheshwari.png HTTP/1.1\u001b[0m\" 404 -\n","INFO:werkzeug:127.0.0.1 - - [09/May/2024 12:56:49] \"GET / HTTP/1.1\" 200 -\n"]}],"source":["if __name__ == \"__main__\":\n","    # !ngrok http 5000\n","    app.run()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}